# Fleet Telemetry Dynamic Partitions

The fleet telemetry scenario demonstrates how dynamic partition keys let a single asset definition track per-device state without hard-coding the list of instruments. Runs discover new partition keys at runtime whenever a previously unseen sensor reports data.

## Sample dataset

The repository ships a small seed dataset under `services/catalog/data/examples/fleet-telemetry/` organised as one folder per instrument. Each folder contains daily CSV extracts named `instrument_<ID>_YYYYMMDD.csv` with hourly readings:

```
services/catalog/data/examples/fleet-telemetry/
  instrument_A/
    instrument_A_20240101.csv
    instrument_A_20240102.csv
  instrument_B/
    instrument_B_20240215.csv
    instrument_B_20240216.csv
  instrument_C/
    instrument_C_20240320.csv
    instrument_C_20240321.csv
```

Every CSV includes `timestamp`, `temperature_c`, `humidity_pct`, and `quality_flag` columns so worker code can derive daily rollups and anomaly spans.

Rollup payloads generated by the metrics job are stored as JSON alongside the example data in `services/catalog/data/examples/fleet-telemetry-rollups/`. The alerts workflow reads from the same directory by default.

## Jobs

| Path | Job slug | Purpose |
| --- | --- | --- |
| `job-bundles/fleet-telemetry-metrics` | `fleet-telemetry-metrics` | Reads an instrument folder for the requested day, aggregates min/max/mean telemetry, writes a JSON rollup to `outputDir` (defaults to `services/catalog/data/examples/fleet-telemetry-rollups`), and emits the `greenhouse.telemetry.instrument` asset keyed by the instrument ID. |
| `job-bundles/greenhouse-alerts-runner` | `greenhouse-alerts-runner` | Scans the rollup directory provided via `telemetryDir`, applies threshold logic, and publishes a `greenhouse.telemetry.alerts` summary asset. |

> The bundle directories can be packaged with `npm run build` from the repo root or with the CLI (`npx tsx apps/cli/src/index.ts jobs package job-bundles/fleet-telemetry-metrics --force`).

## Workflows

Two workflows coordinate ingestion and alerting. Their definitions live in `services/catalog/src/workflows/examples/fleetTelemetryExamples.ts`.

1. **`fleet-telemetry-daily-rollup`** accepts `instrumentId`, `day`, `dataRoot`, and `outputDir`. It runs the metrics job once per device/day pair, producing `greenhouse.telemetry.instrument` with a `dynamic` partition key equal to the instrument ID while persisting a JSON rollup into the configured directory. No prior key registration is neededâ€”the first run for a new instrument persists the key automatically.
2. **`fleet-telemetry-alerts`** monitors the telemetry asset. Whenever a new partition materialises or an existing one refreshes, the auto-materializer replays the alerts workflow. The job scans the rollup directory (default `telemetryDir` matches the rollup output) to raise warnings for out-of-range temperatures, humidity drift, or repeated `quality_flag` anomalies.

## Launching the example

Register each job and workflow using the catalog API (replace `TOKEN` with a valid bearer token):

```bash
# Package and upload the metrics bundle
npx tsx apps/cli/src/index.ts jobs package job-bundles/fleet-telemetry-metrics --force
curl -X POST http://127.0.0.1:4000/jobs \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  --data @tmp/fleet-telemetry-metrics-bundle.json

# Package and upload the alerts bundle
npx tsx apps/cli/src/index.ts jobs package job-bundles/greenhouse-alerts-runner --force
curl -X POST http://127.0.0.1:4000/jobs \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  --data @tmp/greenhouse-alerts-runner-bundle.json

# Register the workflows
node -e "process.stdout.write(JSON.stringify(require('./services/catalog/src/workflows/examples/fleetTelemetryExamples').fleetTelemetryDailyRollupWorkflow, null, 2))" > tmp/fleet-telemetry-daily-rollup.json
node -e "process.stdout.write(JSON.stringify(require('./services/catalog/src/workflows/examples/fleetTelemetryExamples').fleetTelemetryAlertsWorkflow, null, 2))" > tmp/fleet-telemetry-alerts.json

curl -X POST http://127.0.0.1:4000/workflows \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  --data @tmp/fleet-telemetry-daily-rollup.json

curl -X POST http://127.0.0.1:4000/workflows \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  --data @tmp/fleet-telemetry-alerts.json
```

Run the rollup workflow once per instrument/day pair. Passing `partitionKey` on the run payload keeps lineage aligned with the emitted asset key:

```bash
curl -X POST http://127.0.0.1:4000/workflows/fleet-telemetry-daily-rollup/run \
  -H "Authorization: Bearer $TOKEN" \
  -H "Content-Type: application/json" \
  --data '{
    "partitionKey": "instrument_A",
    "parameters": {
      "dataRoot": "services/catalog/data/examples/fleet-telemetry",
      "instrumentId": "instrument_A",
      "day": "2024-01-02"
    }
  }'
```

Repeat for the other instruments (`instrument_B`, `instrument_C`) to add more dynamic partitions. Whenever the rollup workflow succeeds, the auto-materializer enqueues `fleet-telemetry-alerts`, which produces an updated `greenhouse.telemetry.alerts` snapshot summarising all flagged devices.

## Inspecting partitions

Use the catalog API to explore dynamic keys:

```bash
# List known partitions
curl -sS http://127.0.0.1:4000/workflows/fleet-telemetry-daily-rollup/assets/greenhouse.telemetry.instrument/partitions | jq

# Fetch history for a single instrument
curl -sS 'http://127.0.0.1:4000/workflows/fleet-telemetry-daily-rollup/assets/greenhouse.telemetry.instrument/history?partitionKey=instrument_C' | jq
```

As new greenhouse sensors come online, simply drop a folder such as `instrument_D/` into the data root and run the rollup workflow again. The catalog records the new `partitionKey` automatically and keeps the alerts workflow in sync without editing any workflow definitions.
